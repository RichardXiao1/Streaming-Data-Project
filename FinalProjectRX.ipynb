{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc292cc7-3030-41e3-927f-59480eb9786b",
   "metadata": {},
   "source": [
    "# Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ce969-c79b-4a83-821c-036b04c5c4f9",
   "metadata": {},
   "source": [
    "## Richard Xiao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8f527f-1317-451b-b08a-7968af34ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').appName('my_app').getOrCreate()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import avg, stddev,max, percentile_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bfa12-9d4d-4362-ab75-d4a76b9bcb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0caf3f1-13c1-4d27-abb8-31a1338b054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4b4a7-a8c7-4374-9855-061584862477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b80b6e-0cc9-4534-81c5-6f3a54c6b3e7",
   "metadata": {},
   "source": [
    "Code below reads in the power csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf8943-84d5-467a-86f7-8027cf3c978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "power = spark.read.load(\"power_ml_data.csv\",\n",
    "                            format = \"csv\",\n",
    "                            sep = \",\",\n",
    "                            inferSchema = \"true\",\n",
    "                            header = \"true\")  \n",
    "\n",
    "power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7669ab-a5a2-4954-973c-000e2ea1fd27",
   "metadata": {},
   "source": [
    "# Aggregations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de19e554-4ddb-430f-bb15-e444f59d1fde",
   "metadata": {},
   "source": [
    "The code below gives the average of the temperature, humidity, wind speed and the power zones. Looking at it, power zone 1 has the highest average out of the three power zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dff5e3-eac5-4c2e-9692-5bfe87d292d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_mean = power\\\n",
    "               .agg(avg('Temperature'),avg('Humidity'),avg('Wind_Speed'),avg('Power_Zone_1'),avg('Power_Zone_2'),avg('Power_Zone_3'))\\\n",
    "               .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff76d2-275c-484f-8a24-9a6085017954",
   "metadata": {},
   "source": [
    "Code below gives the standard deviaton.Power zone 1 has the highest standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61dfb99-67e5-46d1-ab17-ea63ba7f3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev_power = power\\\n",
    "                 .agg(stddev('Temperature'),stddev('Humidity'),stddev('Wind_Speed'),stddev('Power_Zone_1'),stddev('Power_Zone_2'),stddev('Power_Zone_3'))\\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e24d83e-5109-45a5-a9ae-d0f08a906d68",
   "metadata": {},
   "source": [
    "## Max,Median,and Min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f260da-c36b-4a78-95d1-4684545e4479",
   "metadata": {},
   "source": [
    "For this section, I wanted to get the maximum and minimum values for temperature, humidity and wind speed for each month. It seems that July had the highest max temperature and February had the lowest max value,December for highest humidity and January with the lowest max value, and October had the highest max wind seed and Feb,March and April had the lowest max wind speed values. \n",
    "\n",
    "For the minimum observations, December had the lowest min temp while June had the highest min temp. For minimum humidity, July had the highest min humidity while Feb had the lowest min humidity.For wind speed, July had the lowest min value and May had the highest min value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e0be0-f76e-4020-a856-14a8a12d33a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_power_temp = power.groupby(\"Month\").max(\"Temperature\") \\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470abb13-b6f0-40fe-905d-1ac9309ef0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_power_humidity = power.groupby(\"Month\").max(\"Humidity\") \\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4cd93-982a-4ee0-b960-0d11e9720e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_power_wind = power.groupby(\"Month\").max(\"Wind_Speed\") \\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0203cc3-12b6-41ad-9307-bde77cb17376",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_power_temp = power.groupby(\"Month\").min(\"Temperature\") \\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c256a8-c783-466d-aec3-9c8e21ba674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_power_humidity = power.groupby(\"Month\").min(\"Humidity\") \\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f671cf-6522-4c94-aba3-0005ab15b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_power_wind = power.groupby(\"Month\").min(\"Wind_Speed\") \\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c3f5e-963f-4250-8676-cbad1d2f429d",
   "metadata": {},
   "source": [
    "For the median observations, Power Zone 1 had the highest median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980c037-28f8-4a56-b9fc-4a0996cee6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_power = power\\\n",
    "                 .agg(percentile_approx(\"Temperature\", 0.5).alias(\"median_temp\"),percentile_approx(\"Humidity\", 0.5).alias(\"median_humidity\"),percentile_approx(\"Wind_Speed\", 0.5).alias(\"median_Wind_Speed\"),percentile_approx(\"Power_Zone_1\", 0.5).alias(\"median_power_zone_1\"),percentile_approx(\"Power_Zone_2\", 0.5).alias(\"median_power_zone_2\"),percentile_approx(\"Power_Zone_3\", 0.5).alias(\"median_power_zone_3\")) \\\n",
    "                 .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990f0ae-f6c0-47af-b204-3f7489324c03",
   "metadata": {},
   "source": [
    "## Correlation and Contingency Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e710d-50ff-435f-b55a-d81883e42bc2",
   "metadata": {},
   "source": [
    "The code below will read it in as a spark data frame using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a404b-f22f-4665-9a8d-b99ad7851987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_power=ps.DataFrame(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340d7cc-0d6a-4528-82bb-c2939ad229d3",
   "metadata": {},
   "source": [
    "Next, I selected every column except for hour and month to get the correlation of the numeric variables. Looking at some of the notable results, temperature has the highest correlation with power zone 3, power zone 2 has the highest correlation with power zone 1 and power zone 2 has the highest correlation with power zone 1.Power zone 3 also seems to be highly correlated with power zone 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ad195-6e70-409f-beb2-73cfd6b948cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_power.iloc[:,0:8]\n",
    "df_new.corr('pearson')\n",
    "#df_power.corr('pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726e774e-2684-4d60-ae55-c29e2d8c1a34",
   "metadata": {},
   "source": [
    "The next chunks of code deals with contingency tables. The first line of code is the two way contingency table for month and hour. The next two are one way contingency tables for month and hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d39c88-0fbd-4a7d-b174-92e72b3f8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_temp = power.crosstab(\"month\",\"Hour\")\n",
    "\n",
    "contingency_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5182404-9d6b-4ee1-8580-618bb39e8318",
   "metadata": {},
   "source": [
    "This table shows that March had the highest count of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa1cde-00ad-4d95-8bf6-1480c7ee2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_temp = power.groupBy(\"month\").count()\n",
    "\n",
    "contingency_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635438e2-148c-44af-8a65-97ec18a6c82e",
   "metadata": {},
   "source": [
    "The hour table shows that hour 6 had the most amount of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d278b8b-57be-40df-9aee-2112aa261d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_temp = power.groupBy(\"Hour\").count()\n",
    "\n",
    "contingency_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00980708-c015-4487-af9b-b09887335634",
   "metadata": {},
   "source": [
    "The code chunks belw is finding the average and standard deviation of the numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc18238-d8bd-4081-b0a7-a958d87d00df",
   "metadata": {},
   "source": [
    "According to the averages table, July had the highest average temperature and February the lowest. April had the highest average humidity and July the lowest. For wind speed, April had the lowest avg wind speed and September the highest. August had the highest average value for power zone 1 and December the lowest average. For power zone 2, April had the lowest average and August the highest. And for power zone 3, July had the highest average value and December the lowest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557e729-9c1f-444c-87eb-015fc9baa7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "powermonthmean = power.groupby('month').agg(avg('Temperature'),avg('Humidity'),avg('Wind_Speed'),avg('Power_Zone_1'),avg('Power_Zone_2'),avg('Power_Zone_3')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183ae3c-62d6-4f91-a071-6598dd3a46a5",
   "metadata": {},
   "source": [
    "Based on observations from this table, July had the highest standard deviation for temperature and February for the lowest. For humidity, January had the lowest whileJuly had the highest. May had the highest std for wind speed and April for the lowest. For power zone 1, November had the lowest and January the highest. Power zone 2 had December as the highest standard deviation and April the lowest. And for power zone 3, July had the highest and December the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cada427-7377-475b-b295-93ba70bad908",
   "metadata": {},
   "outputs": [],
   "source": [
    "powermonthstd = power.groupby('month').agg(stddev('Temperature'),stddev('Humidity'),stddev('Wind_Speed'),stddev('Power_Zone_1'),stddev('Power_Zone_2'),stddev('Power_Zone_3')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561f10d-7ffd-46ee-ac68-d825d2840303",
   "metadata": {},
   "source": [
    "This code below converts the hour variable as a double type. This data will be used for the rest of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19ea3c-00cc-4018-8c18-e3ae53798e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_power = power.withColumn(\"Hour\", power.Hour.cast(DoubleType()))\n",
    "test_power.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c0075-b7a6-473c-a20d-efd9b860f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f83d8cc-acef-4b79-b129-7de47f3dd142",
   "metadata": {},
   "source": [
    "## Fitting Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af281a19-26d4-4fd7-9525-40e29be26c5a",
   "metadata": {},
   "source": [
    "These next code chunks will construct the first pipeline where the model is fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e1b0c-5c91-422f-a56c-3668c9647cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0611ee-8eff-40a9-b955-76bd757594c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import SQLTransformer, StringIndexer,Binarizer,VectorAssembler, PolynomialExpansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1605a063-ecc8-4f40-b056-00ed7ed4220f",
   "metadata": {},
   "source": [
    "Binary transformer to convert hour into a binary variable type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38a794-dcde-4f16-b7a4-ae3d24ec3469",
   "metadata": {},
   "outputs": [],
   "source": [
    "binaryhourTrans = Binarizer(threshold = 6.5, inputCols = [\"Hour\"], outputCols = [\"Binary_Hour\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec4d552-33c0-4982-9c66-2d143b7c555a",
   "metadata": {},
   "source": [
    "This code chunk will make an encoder transformer for the month variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd61aaa-91ca-4266-b196-721e6bb2d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotEncoder = OneHotEncoder(inputCol=\"Month\", outputCol=\"month_hot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bd72a-1dd3-45da-bd5e-02021467cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1beea01-c2fc-4011-b614-ac32f52473bd",
   "metadata": {},
   "source": [
    "Compile the assembler for the PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7343f-11ff-44da-ab89-e8438d296d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_pca = VectorAssembler(inputCols = [\"Temperature\", \"Humidity\",\"Wind_Speed\",\"General_Diffuse_Flows\",\"Diffuse_Flows\"], outputCol = \"features\", handleInvalid = 'keep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed1be6-0673-4bfe-be4a-37c08f91a173",
   "metadata": {},
   "source": [
    "Construct PCA component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486acff9-af01-4f41-ab60-d5dfbe3ecd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(k = 3, inputCol = \"features\", outputCol= \"features_pca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f83db-4cdf-421a-9775-05805f4fe7e1",
   "metadata": {},
   "source": [
    "Construct the second assembler to serve as the predictors for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db549d54-0a1e-4bf3-bc60-abd91790995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_new = VectorAssembler(inputCols = [\"features_pca\", \"Binary_Hour\",\"Power_Zone_1\",\"Power_Zone_2\",\"month_hot\"], outputCol = \"features_new\", handleInvalid = 'keep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d9305-58bc-48db-abfe-bffa2e4a9cc9",
   "metadata": {},
   "source": [
    "This sql transformer will select only the new features and the response variable, which is power zone 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367f02a-809b-4ccf-95bf-46fe865ffd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT features_new, Power_Zone_3 as label FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96f703-432c-40ec-ab38-3b5a5f04c5ed",
   "metadata": {},
   "source": [
    "Elastic net object is declared below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1eba7e-264e-47f2-a9ff-4729acae120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = LinearRegression(featuresCol=\"features_new\", labelCol=\"label\", elasticNetParam=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97209e33-4964-4c08-9014-c58eedc81915",
   "metadata": {},
   "source": [
    "Pipeline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e9c84-ca1d-4f26-a264-dd32b0dafec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [binaryhourTrans,oneHotEncoder,assembler_pca,pca,assembler_new,sqlTrans,elastic_net])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4e70d-94d6-469c-b692-7ad217ed3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c80075f-5593-400d-87c9-290d4fd62fa9",
   "metadata": {},
   "source": [
    "These next three code chunks sets up the grid for regParam and elasticNetParam, the cross validator and will fit the model against the new power data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca506b-e1a2-4424-8a87-187d46ad7054",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(elastic_net.regParam, [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1]) \\\n",
    "    .addGrid(elastic_net.elasticNetParam, [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.98, 0.99, 1]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c525e-3562-455c-9138-abbdbe940bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=RegressionEvaluator(), numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9628532-6910-4035-8ea3-3b8336735a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(test_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603437b4-6ff2-4a9f-888b-3ca109973289",
   "metadata": {},
   "source": [
    "Sets up the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d160e1-fb19-4307-8643-b3ba5b866e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cvModel.transform(test_power)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78673ae6-e259-4fa2-bdd7-e540deba9589",
   "metadata": {},
   "source": [
    "Declares rsme value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9353e2b-be22-4be5-8743-a4cdda5a2f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = RegressionEvaluator(metricName=\"rmse\").evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaaacf4-c12e-49c6-920b-0ea2e50e1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE: {:.2f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b46af-3cda-422c-b79e-fb1a4e62d707",
   "metadata": {},
   "source": [
    "Create a new column which is the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62397878-b8d0-48b9-a6ae-3bb7e072f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = predictions.withColumn(\"residual\",col(\"label\") - col(\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b731452-69c2-4301-a024-75744a43a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7baec-c4b7-4629-9283-79d333a0ce95",
   "metadata": {},
   "source": [
    "# Streaming Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978fe104-58fd-46cb-8230-b206412f4e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a75c7a7-cf79-45a1-b5fc-efa6a7fc357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newpower_stream = spark.read.load(\"power_streaming_data.csv\",\n",
    "                            format = \"csv\",\n",
    "                            sep = \",\",\n",
    "                            inferSchema = \"true\",\n",
    "                            header = \"true\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b68c4c-360a-4e30-bca9-e917f23a7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "##spark = SparkSession.builder.master(\"local(*)\").appName(\"power_stream\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e5335-43cd-4716-bef6-20d0fb08facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29fd68-0535-4eab-8eb0-caf3ac6dfc4b",
   "metadata": {},
   "source": [
    "Sets up the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95489a-b647-4626-9a01-a4bd6a1f525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = StructType([ \\\n",
    "    StructField(\"Temperature\", DoubleType(), True), \\\n",
    "    StructField(\"Humidity\", DoubleType(), True), \\\n",
    "    StructField(\"Wind_Speed\" ,DoubleType(), True), \\\n",
    "    StructField(\"General_Diffuse_Flows\", DoubleType(), True), \\\n",
    "    StructField(\"Diffuse_Flows\", DoubleType(), True), \\\n",
    "    StructField(\"Power_Zone_1\", DoubleType(), True), \\\n",
    "    StructField(\"Power_Zone_2\", DoubleType(), True), \\\n",
    "    StructField(\"Power_Zone_3\", DoubleType(), True), \\\n",
    "    StructField(\"Month\", DoubleType(), True), \\\n",
    "    StructField(\"Hour\", DoubleType(), True), \\\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599aa7f-1f63-46ad-9ec0-5feaec46b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00dce49-dd1d-4430-b647-e76920eb8a54",
   "metadata": {},
   "source": [
    "Code below sets up folder to output csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e17c00-51e9-4660-a549-73d8ec73fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamread = spark.readStream.option(\"header\", \"True\").schema(myschema).csv(\"csv_streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8af8a-5169-4f14-abfc-747c8255ee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d1457-0472-4998-bc8e-16d381a4912e",
   "metadata": {},
   "source": [
    "## Transfrom/Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de489ac-2278-4a47-8545-5022fce975f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import explode, split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d392139-8f53-48dd-9c2f-5f40c58e9879",
   "metadata": {},
   "source": [
    "Sets up the second pipeline function to join. For this part, I reused most of my first pipeline transformers. The only different thing is I added in a second sql transformer which selects all of the data instead of only the new features and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd184ddc-fc43-45c7-8120-bb1ffd491aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlTrans2 = SQLTransformer(\n",
    "    statement = \"\"\"\n",
    "                SELECT *,Power_Zone_3 as label FROM __THIS__\n",
    "                \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f535c007-e930-4503-a09d-85b706cf198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline2 = Pipeline(stages = [oneHotEncoder,binaryhourTrans,assembler_pca,pca, assembler_new,sqlTrans2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f90298-a9ba-4ffe-9b81-539bc95fbad6",
   "metadata": {},
   "source": [
    "These next code chunks will transform the stream data using the first pipeline. This is the first stream of data to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2b9ca-04be-4d7d-9a90-b3474db5c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_stream = cvModel.transform(streamread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741d919-b4b3-4444-ae5e-ce176d396331",
   "metadata": {},
   "outputs": [],
   "source": [
    "newstream = cvModel.transform(streamread).withColumn(\"residuals\", col(\"label\") - col(\"prediction\")) #first thing you want to join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7e6fb-99aa-49b6-a733-efc55205dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "newstream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d17ae-712d-4b55-b513-fdafea5d6b9c",
   "metadata": {},
   "source": [
    "Second pipeline is used to use for the second stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d7c8c-3252-4a1c-94d7-27821d18b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "newstream2=pipeline2.fit(test_power).transform(streamread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d091f556-1d4f-4bfb-a898-460f07fc70e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "newstream2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63943f01-370f-4bb2-8982-2b033b6ac305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d3a905-532c-4559-9518-2acc447e6f42",
   "metadata": {},
   "source": [
    "Code chunk below will join the two data streams together on the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0380a2-bdec-4186-9087-80a9a5c04508",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinquery = newstream.join(newstream2,\"label\", \"inner\").writeStream.outputMode(\"append\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa86affb-9032-46d2-bb7f-e05da6b4edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05bb2e-d3de-47fa-a046-a8931ef20952",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinquery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8301a-a00c-4e3b-bdc8-6b6fffc03712",
   "metadata": {},
   "source": [
    "## Produce Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e6316-3beb-4e79-a88c-67ffb385792b",
   "metadata": {},
   "source": [
    "The code chunk below will be submitted to the console and output various csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddd3c2d-980b-4ed3-b187-4ebbf585f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_stream = pd.read_csv(\"csv_streaming/power_streaming_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a952fa-1204-4d6c-b2f3-80752a28c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd23c5-6ca8-456f-9cbe-8e2c5c168a33",
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(0,50):\n",
    "    temp = power_stream.loc[np.random.randint(power_stream.shape[0], size = 3)]\n",
    "    temp.to_csv(\"csv_streaming/power_stream\" + str(i) + \".csv\", index = False, header = True)\n",
    "    time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
